pytest tests/keras/engine/test_training.py::test_model_methods
RUN EVERY COMMAND
0


============================= test session starts ==============================
platform linux -- Python 3.7.3, pytest-5.4.3, py-1.8.1, pluggy-0.13.1 -- /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/bin/python
cachedir: .pytest_cache
rootdir: /home/user/BugsInPy/temp/projects/keras, inifile: pytest.ini
plugins: forked-1.1.3, flaky-3.6.1, xdist-1.32.0
gw0 I / gw1 I
[gw0] linux Python 3.7.3 cwd: /home/user/BugsInPy/temp/projects/keras
[gw1] linux Python 3.7.3 cwd: /home/user/BugsInPy/temp/projects/keras
[gw0] Python 3.7.3 (default, Mar 27 2019, 22:11:17)  -- [GCC 7.3.0]
[gw1] Python 3.7.3 (default, Mar 27 2019, 22:11:17)  -- [GCC 7.3.0]
gw0 [1] / gw1 [1]

scheduling tests via LoadScheduling

tests/keras/engine/test_training.py::test_model_methods 
[gw0] [100%] FAILED tests/keras/engine/test_training.py::test_model_methods 

=================================== FAILURES ===================================
______________________________ test_model_methods ______________________________
[gw0] linux -- Python 3.7.3 /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/bin/python

    def test_model_methods():
        a = Input(shape=(3,), name='input_a')
        b = Input(shape=(3,), name='input_b')
    
        a_2 = Dense(4, name='dense_1')(a)
        dp = Dropout(0.5, name='dropout')
        b_2 = dp(b)
    
        model = Model([a, b], [a_2, b_2])
    
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        # training/testing doesn't work before compiling.
        with pytest.raises(RuntimeError):
            model.train_on_batch([input_a_np, input_b_np],
                                 [output_a_np, output_b_np])
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
    
        # test train_on_batch
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # test fit
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4)
    
        # test validation_split
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
    
        # test validation data
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4,
                        validation_data=([input_a_np, input_b_np],
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=({'input_a': input_a_np,
                                          'input_b': input_b_np},
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=(
                            {'input_a': input_a_np, 'input_b': input_b_np},
                            {'dense_1': output_a_np, 'dropout': output_b_np}))
    
        # test_on_batch
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # predict_on_batch
        out = model.predict_on_batch([input_a_np, input_b_np])
        out = model.predict_on_batch({'input_a': input_a_np,
                                      'input_b': input_b_np})
    
        # predict, evaluate
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # with sample_weight
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        sample_weight = [None, np.random.random((10,))]
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np],
                                   sample_weight=sample_weight)
    
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np],
                                  sample_weight=sample_weight)
    
        # test accuracy metric
        model.compile(optimizer, loss, metrics=['acc'],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 5
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 5
    
        # this should also work
        model.compile(optimizer, loss, metrics={'dense_1': 'acc'},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        # and this as well
        model.compile(optimizer, loss, metrics={'dense_1': ['acc']},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        # test starting from non-zero initial epoch
        trained_epochs = []
        trained_batches = []
    
        # define tracer callback
        def on_epoch_begin(epoch, logs):
            trained_epochs.append(epoch)
    
        def on_batch_begin(batch, logs):
            trained_batches.append(batch)
    
        tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,
                                    on_batch_begin=on_batch_begin)
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=5, batch_size=4,
                        initial_epoch=2, callbacks=[tracker_cb])
        assert trained_epochs == [2, 3, 4]
    
        # test starting from non-zero initial epoch for generator too
        trained_epochs = []
    
        @threadsafe_generator
        def gen_data(batch_sz):
            while True:
                yield ([np.random.random((batch_sz, 3)),
                        np.random.random((batch_sz, 3))],
                       [np.random.random((batch_sz, 4)),
                        np.random.random((batch_sz, 3))])
    
        out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,
                                  initial_epoch=2, callbacks=[tracker_cb])
        assert trained_epochs == [2, 3, 4]
    
        # test with a custom metric function
        def mse(y_true, y_pred):
            return K.mean(K.pow(y_true - y_pred, 2))
    
        model.compile(optimizer, loss, metrics=[mse],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)
        assert len(out) == out_len
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == out_len
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        batch_size=4, epochs=1)
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # enable verbose for evaluate_generator
        out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)
    
        # empty batch
        with pytest.raises(ValueError):
            @threadsafe_generator
            def gen_data():
                while True:
                    yield (np.asarray([]), np.asarray([]))
    
            out = model.evaluate_generator(gen_data(), steps=1)
    
        # x is not a list of numpy arrays.
        with pytest.raises(ValueError):
            out = model.predict([None])
    
        # x does not match _feed_input_names.
        with pytest.raises(ValueError):
            out = model.predict([input_a_np, None, input_b_np])
        with pytest.raises(ValueError):
            out = model.predict([None, input_a_np, input_b_np])
    
        # all input/output/weight arrays should have the same number of samples.
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np[:2]],
                                       [output_a_np, output_b_np],
                                       sample_weight=sample_weight)
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np[:2]],
                                       sample_weight=sample_weight)
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np],
                                       sample_weight=[sample_weight[1],
                                                      sample_weight[1][:2]])
    
        # `sample_weight` is neither a dict nor a list.
        with pytest.raises(TypeError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np],
                                       sample_weight=tuple(sample_weight))
    
        # `validation_data` is neither a tuple nor a triple.
        with pytest.raises(ValueError):
            out = model.fit([input_a_np, input_b_np],
                            [output_a_np, output_b_np],
                            epochs=1, batch_size=4,
                            validation_data=([input_a_np, input_b_np],))
    
        # `loss` does not match outputs.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss=['mse', 'mae', 'mape'])
    
        # `loss_weights` does not match output_names.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})
    
        # `loss_weights` does not match outputs.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse', loss_weights=[0.5])
    
        # `loss_weights` is invalid type.
        with pytest.raises(TypeError):
            model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))
    
        # `sample_weight_mode` does not match output_names.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse',
                          sample_weight_mode={'lstm': 'temporal'})
    
        # `sample_weight_mode` does not match output_names.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])
    
        # `sample_weight_mode` matches output_names partially.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse',
                          sample_weight_mode={'dense_1': 'temporal'})
    
        # `loss` does not exist.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss=[])
    
        model.compile(optimizer, loss=['mse', 'mae'])
        model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,
                                                           'dropout': 0.8})
        model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])
    
        # the rank of weight arrays should be 1.
        with pytest.raises(ValueError):
            out = model.train_on_batch(
                [input_a_np, input_b_np],
                [output_a_np, output_b_np],
                sample_weight=[None, np.random.random((10, 20, 30))])
    
        model.compile(optimizer, loss='mse',
                      sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})
        model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])
    
        # the rank of output arrays should be at least 3D.
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np],
                                       sample_weight=sample_weight)
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
        trained_epochs = []
        trained_batches = []
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  steps_per_epoch=3,
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  validation_steps=3,
                                  max_queue_size=1,
                                  callbacks=[tracker_cb])
        assert trained_epochs == [0, 1, 2, 3, 4]
        assert trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        # steps_per_epoch will be equal to len of sequence if it's unspecified
        trained_epochs = []
        trained_batches = []
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb])
        assert trained_epochs == [0, 1, 2, 3, 4]
        assert trained_batches == list(range(12)) * 5
        assert len(val_seq.logs) == 12 * 5
    
        # test for workers = 0
        trained_epochs = []
        trained_batches = []
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
>                                 workers=0)

tests/keras/engine/test_training.py:479: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/legacy/interfaces.py:91: in wrapper
    return func(*args, **kwargs)
keras/engine/training.py:1418: in fit_generator
    initial_epoch=initial_epoch)
keras/engine/training_generator.py:233: in fit_generator
    workers=0)
keras/legacy/interfaces.py:91: in wrapper
    return func(*args, **kwargs)
keras/engine/training.py:1472: in evaluate_generator
    verbose=verbose)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = <keras.engine.training.Model object at 0x7f0ca0844a90>
generator = <generator object iter_sequence_infinite at 0x7f0ca0a76840>
steps = None, max_queue_size = 10, workers = 0, use_multiprocessing = False
verbose = 0

    def evaluate_generator(model, generator,
                           steps=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
                           verbose=0):
        """See docstring for `Model.evaluate_generator`."""
        model._make_test_function()
    
        if hasattr(model, 'metrics'):
            for m in model.stateful_metric_functions:
                m.reset_states()
            stateful_metric_indices = [
                i for i, name in enumerate(model.metrics_names)
                if str(name) in model.stateful_metric_names]
        else:
            stateful_metric_indices = []
    
        steps_done = 0
        wait_time = 0.01
        outs_per_batch = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if is_sequence:
                steps = len(generator)
            else:
>               raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
E               ValueError: `steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class.

keras/engine/training_generator.py:300: ValueError
----------------------------- Captured stdout call -----------------------------
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 1.0782 - dense_1_loss: 0.8449 - dropout_loss: 0.4666
10/10 [==============================] - 0s 1ms/step - loss: 0.9060 - dense_1_loss: 0.6863 - dropout_loss: 0.4394
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.6729 - dense_1_loss: 0.4543 - dropout_loss: 0.4372
10/10 [==============================] - 0s 385us/step - loss: 1.0240 - dense_1_loss: 0.6706 - dropout_loss: 0.7067
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 1.0185 - dense_1_loss: 0.8247 - dropout_loss: 0.3876
10/10 [==============================] - 0s 2ms/step - loss: 0.9082 - dense_1_loss: 0.6577 - dropout_loss: 0.5011
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.7268 - dense_1_loss: 0.4431 - dropout_loss: 0.5674
5/5 [==============================] - 0s 15ms/step - loss: 0.7023 - dense_1_loss: 0.4487 - dropout_loss: 0.5073 - val_loss: 0.9505 - val_dense_1_loss: 0.8427 - val_dropout_loss: 0.2154
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.6208 - dense_1_loss: 0.4168 - dropout_loss: 0.4081
5/5 [==============================] - 0s 468us/step - loss: 0.6494 - dense_1_loss: 0.4439 - dropout_loss: 0.4110 - val_loss: 0.9438 - val_dense_1_loss: 0.8361 - val_dropout_loss: 0.2154
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 1.0281 - dense_1_loss: 0.7701 - dropout_loss: 0.5161
10/10 [==============================] - 0s 338us/step - loss: 0.9289 - dense_1_loss: 0.6353 - dropout_loss: 0.5873 - val_loss: 0.7194 - val_dense_1_loss: 0.6275 - val_dropout_loss: 0.1837
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.8974 - dense_1_loss: 0.7500 - dropout_loss: 0.2949
10/10 [==============================] - 0s 323us/step - loss: 0.7866 - dense_1_loss: 0.6250 - dropout_loss: 0.3232 - val_loss: 0.7094 - val_dense_1_loss: 0.6176 - val_dropout_loss: 0.1837
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.5788 - dense_1_loss: 0.3975 - dropout_loss: 0.3627
10/10 [==============================] - 0s 1ms/step - loss: 0.8464 - dense_1_loss: 0.6149 - dropout_loss: 0.4630 - val_loss: 0.6987 - val_dense_1_loss: 0.6068 - val_dropout_loss: 0.1837

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 1ms/step
Epoch 3/5

 4/10 [===========>..................] - ETA: 0s - loss: 1.5069 - dense_1_loss: 1.1515 - dropout_loss: 0.3555 - dense_1_acc: 0.0000e+00
10/10 [==============================] - 0s 201us/step - loss: 1.2259 - dense_1_loss: 0.7304 - dropout_loss: 0.4955 - dense_1_acc: 0.1000
Epoch 4/5

 4/10 [===========>..................] - ETA: 0s - loss: 1.2554 - dense_1_loss: 0.6097 - dropout_loss: 0.6457 - dense_1_acc: 0.2500
10/10 [==============================] - 0s 167us/step - loss: 1.1998 - dense_1_loss: 0.7139 - dropout_loss: 0.4859 - dense_1_acc: 0.1000
Epoch 5/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.9116 - dense_1_loss: 0.5173 - dropout_loss: 0.3943 - dense_1_acc: 0.0000e+00
10/10 [==============================] - 0s 167us/step - loss: 1.2833 - dense_1_loss: 0.7010 - dropout_loss: 0.5824 - dense_1_acc: 0.1000
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8143 - dense_1_loss: 0.2503 - dropout_loss: 0.5640 - dense_1_acc: 0.0000e+00
3/3 [==============================] - 0s 7ms/step - loss: 1.3456 - dense_1_loss: 0.7326 - dropout_loss: 0.6130 - dense_1_acc: 0.1667
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0001 - dense_1_loss: 0.4140 - dropout_loss: 0.5861 - dense_1_acc: 0.2500
3/3 [==============================] - 0s 7ms/step - loss: 1.3001 - dense_1_loss: 0.6916 - dropout_loss: 0.6085 - dense_1_acc: 0.5000
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0902 - dense_1_loss: 0.5695 - dropout_loss: 0.5207 - dense_1_acc: 0.2500
3/3 [==============================] - 0s 843us/step - loss: 1.1416 - dense_1_loss: 0.5878 - dropout_loss: 0.5538 - dense_1_acc: 0.1667
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.9214 - dense_1_loss: 0.4427 - dropout_loss: 0.4787 - dense_1_mse: 0.4427 - dropout_mse: 0.4787
10/10 [==============================] - 0s 1ms/step - loss: 1.0606 - dense_1_loss: 0.5339 - dropout_loss: 0.5267 - dense_1_mse: 0.5339 - dropout_mse: 0.5267

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 101us/step

1/3 [=========>....................] - ETA: 0s
3/3 [==============================] - 0s 5ms/step
Epoch 1/5

1/3 [=========>....................] - ETA: 1s - loss: 1.0200 - dense_1_loss: 0.7932 - dropout_loss: 0.4535
3/3 [==============================] - 1s 267ms/step - loss: 0.9972 - dense_1_loss: 0.7650 - dropout_loss: 0.4644 - val_loss: 0.6696 - val_dense_1_loss: 0.5669 - val_dropout_loss: 0.2055
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 1.1016 - dense_1_loss: 0.5297 - dropout_loss: 1.1437
3/3 [==============================] - 0s 1ms/step - loss: 0.8410 - dense_1_loss: 0.5286 - dropout_loss: 0.6249 - val_loss: 0.8324 - val_dense_1_loss: 0.7629 - val_dropout_loss: 0.1388
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.4373 - dense_1_loss: 0.2112 - dropout_loss: 0.4521
3/3 [==============================] - 0s 7ms/step - loss: 0.6064 - dense_1_loss: 0.3977 - dropout_loss: 0.4173 - val_loss: 0.9674 - val_dense_1_loss: 0.8543 - val_dropout_loss: 0.2262
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7513 - dense_1_loss: 0.6000 - dropout_loss: 0.3028
3/3 [==============================] - 0s 5ms/step - loss: 0.8512 - dense_1_loss: 0.6557 - dropout_loss: 0.3911 - val_loss: 0.7176 - val_dense_1_loss: 0.6103 - val_dropout_loss: 0.2147
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8742 - dense_1_loss: 0.5716 - dropout_loss: 0.6052
3/3 [==============================] - 0s 37ms/step - loss: 0.8369 - dense_1_loss: 0.5515 - dropout_loss: 0.5709 - val_loss: 0.6706 - val_dense_1_loss: 0.5773 - val_dropout_loss: 0.1865
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.8565 - dense_1_loss: 0.5612 - dropout_loss: 0.5907
 5/12 [===========>..................] - ETA: 0s - loss: 0.7895 - dense_1_loss: 0.5169 - dropout_loss: 0.5453
12/12 [==============================] - 0s 11ms/step - loss: 0.7347 - dense_1_loss: 0.4727 - dropout_loss: 0.5240 - val_loss: 0.6802 - val_dense_1_loss: 0.5844 - val_dropout_loss: 0.1915
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 1.0174 - dense_1_loss: 0.7099 - dropout_loss: 0.6151
12/12 [==============================] - 0s 10ms/step - loss: 0.8505 - dense_1_loss: 0.5927 - dropout_loss: 0.5156 - val_loss: 0.5632 - val_dense_1_loss: 0.4711 - val_dropout_loss: 0.1841
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.7463 - dense_1_loss: 0.5268 - dropout_loss: 0.4390
12/12 [==============================] - 0s 7ms/step - loss: 0.7836 - dense_1_loss: 0.5443 - dropout_loss: 0.4785 - val_loss: 0.6757 - val_dense_1_loss: 0.5987 - val_dropout_loss: 0.1540
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6755 - dense_1_loss: 0.4811 - dropout_loss: 0.3887
12/12 [==============================] - 0s 9ms/step - loss: 0.6410 - dense_1_loss: 0.4230 - dropout_loss: 0.4358 - val_loss: 0.5837 - val_dense_1_loss: 0.5010 - val_dropout_loss: 0.1654
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.4936 - dense_1_loss: 0.3931 - dropout_loss: 0.2010
12/12 [==============================] - 0s 10ms/step - loss: 0.6831 - dense_1_loss: 0.4509 - dropout_loss: 0.4644 - val_loss: 0.5069 - val_dense_1_loss: 0.4276 - val_dropout_loss: 0.1585
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.8694 - dense_1_loss: 0.4631 - dropout_loss: 0.8126
----------------------------- Captured stderr call -----------------------------
WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

------------------------------ Captured log call -------------------------------
WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING  tensorflow:deprecation.py:506 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.
--------------------------- Captured stderr teardown ---------------------------
WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

---------------------------- Captured log teardown -----------------------------
WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.
=============================== warnings summary ===============================
tests/keras/engine/test_training.py:1104
  /home/user/BugsInPy/temp/projects/keras/tests/keras/engine/test_training.py:1104: DeprecationWarning: invalid escape sequence \d
    'have one entry per model output. The model has \d '

tests/keras/engine/test_training.py:1109
  /home/user/BugsInPy/temp/projects/keras/tests/keras/engine/test_training.py:1109: DeprecationWarning: invalid escape sequence \d
    match='The model has \d outputs, but you passed a single '

/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
  /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
    tensor_proto.tensor_content = nparray.tostring()

/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:339
  /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:339: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    if not isinstance(values, collections.Sequence):

keras/utils/data_utils.py:651
keras/utils/data_utils.py:651
keras/utils/data_utils.py:651
  /home/user/BugsInPy/temp/projects/keras/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.
    DeprecationWarning)

/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/generator_io.py:26
  /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/generator_io.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Container

-- Docs: https://docs.pytest.org/en/latest/warnings.html
========================== slowest 20 test durations ===========================
5.76s call     tests/keras/engine/test_training.py::test_model_methods

(0.00 durations hidden.  Use -vv to show these durations.)
=========================== short test summary info ============================
FAILED tests/keras/engine/test_training.py::test_model_methods - ValueError: ...
======================= 1 failed, 14 warnings in 17.26s ========================
Using TensorFlow backend.
